# ðŸ“‹ AuditorÃ­a de AI_WORKFLOW.md â€” Cultura AI-First

**Archivo evaluado**: `AI_WORKFLOW.md`  
**Fecha**: 12 de febrero de 2026  

---

## CalificaciÃ³n Global: 3.9 / 5

| Criterio | Peso | Nota | Ponderado |
|----------|------|------|-----------|
| Rol de la IA definido | 25% | 4.0 | 1.00 |
| IteraciÃ³n de prompting | 25% | 3.5 | 0.88 |
| Errores de IA documentados | 25% | 4.5 | 1.13 |
| Protocolos/metodologÃ­a | 25% | 3.5 | 0.88 |
| **TOTAL** | | | **3.9 / 5** |

---

## 1. Â¿Define claramente el rol de la IA? â€” 4/5

### Lo que hace bien:
- Tabla explÃ­cita IA=Developer vs Humano=Arquitecto (secciÃ³n 1.1)
- 5 reglas de oro con lÃ­mites claros
- Capacidades del agente documentadas (Docker, psql, git)
- Scope definido ("alcance MVP, sin idempotencia, sin health checks propios")

### Lo que falta:
- No define **cuÃ¡ndo NO usar IA**. Â¿Schema SQL? Â¿Decisiones de exchange type? Â¿compose.yml? El documento implica que todo pasa por IA, pero la tabla de decisiones (secciÃ³n 7) muestra que el humano definiÃ³ enums nativos, exchange topic, etc. sin la IA.
- No distingue entre "IA genera desde cero" vs "IA refactoriza cÃ³digo existente" vs "IA debuggea". Son interacciones muy distintas con resultados predecibles diferentes.

---

## 2. Â¿Describe iteraciÃ³n de prompting? â€” 3.5/5

### Lo que hace bien:
- Ciclo visual: `Definir â†’ Prompt â†’ Revisar â†’ Probar â†’ Corregir â†’ Commit`
- SecciÃ³n 3.1 documenta fragmentaciÃ³n del trabajo (Consumer â†’ Service â†’ Repository â†’ Tests)
- Regla "un objetivo por prompt"
- Prompt de contextualizaciÃ³n inicial completo (secciÃ³n 4.1)

### Lo que falta:
- **No hay prompts reales**. El documento tiene UN prompt ejemplo (el de contextualizaciÃ³n), pero no muestra los prompts iterativos que causaron los fixes. Â¿CÃ³mo se le pidiÃ³ a la IA que corrigiera el error `25P02`? Â¿QuÃ© prompt generÃ³ el dispatcher con `EndsWith`?
- No documenta **tÃ©cnicas de prompting**: Â¿se usÃ³ few-shot? Â¿Se pegÃ³ el stack trace literal? Â¿Se daba contexto del error o solo "arrÃ©glalo"?
- La secciÃ³n 5.2 dice "pedir explicaciones" pero no muestra ejemplos de cuÃ¡ndo eso cambiÃ³ el resultado.

---

## 3. Â¿Documenta errores de la IA? â€” 4.5/5

### Lo que hace bien:
- Tabla de 6 bugs encadenados en Payment Service (secciÃ³n 3.2) â€” **excelente**, con causa raÃ­z y fix especÃ­fico
- Error del `SectionId` inexistente documentado (secciÃ³n 3.1)
- Admite que la IA no detectÃ³ problemas de raw SQL + change tracker
- Reconoce que la IA proponÃ­a funcionalidades fuera de scope
- Regla 4 documenta rechazo de credenciales hardcodeadas

### Lo que falta:
- Los errores documentados son solo del ReservationService y Payment Service. Â¿El Producer y el CRUD Service no tuvieron errores? Si no, eso tambiÃ©n es informaciÃ³n relevante.
- No categoriza los errores: Â¿fueron de modelo (schema incorrecto)? Â¿De lÃ³gica? Â¿De infraestructura? Un patrÃ³n ayudarÃ­a a predecir dÃ³nde fallarÃ¡ la IA en el futuro.

---

## 4. Â¿Define protocolos o metodologÃ­a? â€” 3.5/5

### Lo que hace bien:
- Workflow pre/durante/post sesiÃ³n (secciones 5.1-5.3)
- Convenciones de comentarios HUMAN CHECK y AI-GENERATED (secciÃ³n 6)
- Tabla de decisiones con fecha y responsable (secciÃ³n 7)
- Documentos que se comparten al inicio (secciÃ³n 4)

### Lo que falta:
- **No hay criterios de aceptaciÃ³n cuantificables**. "Revisar output" no es un protocolo. Â¿QuÃ© se revisa? Â¿CompilaciÃ³n? Â¿Tests pasan? Â¿Query plan? Â¿Memory leaks?
- **No define rollback**. Â¿QuÃ© pasa cuando la IA genera 3 iteraciones incorrectas seguidas? Â¿Se descarta todo y se reescribe a mano? Â¿Se cambia de herramienta? La secciÃ³n 3.2 muestra que se iterÃ³ 6 veces, pero no dice si hubo un lÃ­mite.
- **No define ownership de archivos compartidos**. `schema.sql` y `compose.yml` los mencionan como "fuente de verdad" pero Â¿quiÃ©n los edita? Â¿La IA puede proponer cambios al schema?
- La convenciÃ³n `// AI-GENERATED` aparece definida pero **no existe una sola instancia** en el cÃ³digo real (solo hay `HUMAN CHECK`).

---

## JustificaciÃ³n TÃ©cnica del 3.9

**Por quÃ© no es 5**: Un flujo AI-First maduro requiere **trazabilidad completa del prompting** â€” sin los prompts reales, no se puede reproducir ni auditar el proceso. El documento describe *quÃ© se hizo* pero no *cÃ³mo se le pidiÃ³ a la IA*. TambiÃ©n falta la convenciÃ³n `AI-GENERATED` aplicada en el cÃ³digo (definida pero no usada), y no hay criterios cuantificables de aceptaciÃ³n.

**Por quÃ© no es 3**: La tabla de 6 bugs encadenados es evidencia concreta de iteraciÃ³n humano-IA que pocos equipos documentan. El prompt de contextualizaciÃ³n, las capacidades del agente, y el registro de decisiones muestran un proceso intencionado, no improvisado. La regla de scope (indicarle al agente quÃ© microservicio le corresponde) es una prÃ¡ctica madura.

**Lo mÃ¡s fuerte**: SecciÃ³n 3.2 (6 bugs encadenados con causa raÃ­z y fix). Es la evidencia mÃ¡s honesta de cÃ³mo trabaja realmente un equipo con IA â€” no "la IA lo generÃ³ perfecto", sino "iteramos 6 veces hasta que funcionÃ³".

**Lo mÃ¡s dÃ©bil**: Ausencia total de prompts reales. Para un evaluador externo, es imposible saber si los prompts fueron sofisticados ("aquÃ­ estÃ¡ el stack trace, el schema y el compose; analiza la incompatibilidad de tipos entre Npgsql y PostgreSQL enum") o genÃ©ricos ("fix this error").

---

## Recomendaciones para subir a 5/5

### R1: Agregar prompts reales (impacto alto)
Incluir al menos 3 prompts textuales de sesiones de debugging reales. Ejemplo:

```
PROMPT REAL (Bug #4 - error 25P02):
"El payment consumer falla con PostgreSQL error 25P02 'current transaction is aborted'.
El stack trace apunta a TicketStateService.TransitionToPaidAsync.
Adjunto: schema.sql (enums nativos), el cÃ³digo del service, y el log completo.
Â¿El problema es que .ToString().ToLower() convierte el enum a texto plano
en vez de usar el tipo nativo de PostgreSQL?"

RESPUESTA IA: [resumen de lo que sugiriÃ³]
RESULTADO: Fix correcto, se integrÃ³.
```

### R2: Categorizar errores de la IA
Agregar una tabla resumen:

| CategorÃ­a | Errores | Ejemplo |
|-----------|---------|---------|
| Schema mismatch | 2 | SectionId inexistente, campos de DTO |
| Tipo de dato | 1 | ToString() en enum nativo |
| Concurrencia | 2 | Version pre-increment, change tracker |
| Arquitectura | 1 | Dispatcher match exacto vs EndsWith |

### R3: Definir criterios de aceptaciÃ³n
```
Antes de integrar cÃ³digo generado por IA:
â–¡ Compila sin warnings
â–¡ Docker Compose up exitoso
â–¡ Flujo completo funciona (reserva â†’ pago â†’ verificaciÃ³n en BD)
â–¡ No hay queries N+1 visibles en logs
â–¡ HUMAN CHECK en toda lÃ³gica de concurrencia y mensajerÃ­a
```

### R4: Agregar protocolo de rollback
```
Si la IA falla 3 iteraciones consecutivas en el mismo bug:
1. Parar y analizar el problema manualmente
2. Escribir el fix humano como pseudo-cÃ³digo
3. Pedir a la IA que implemente el pseudo-cÃ³digo (no que diagnostique)
```

### R5: Usar la convenciÃ³n AI-GENERATED que definieron
La secciÃ³n 6.2 define `// AI-GENERATED` pero no hay una sola instancia en el cÃ³digo. Esto debilita el protocolo porque sugiere que se definiÃ³ pero no se cumpliÃ³.

---

**Auditor**: EvaluaciÃ³n AI_WORKFLOW  
**Fecha**: 12 de febrero de 2026
